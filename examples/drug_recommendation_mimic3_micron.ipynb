{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dfd015",
   "metadata": {},
   "source": [
    "# Drug Recommendation using MICRON Model on MIMIC-III Dataset\n",
    "\n",
    "This notebook demonstrates how to use the MICRON model for drug recommendation using the MIMIC-III dataset. The model is implemented using PyHealth 2.0 framework.\n",
    "\n",
    "MICRON (Medication reCommendation using Recurrent ResIdual Networks) is designed to predict medications based on patient diagnoses and procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46345c0b",
   "metadata": {},
   "source": [
    "## 1. Setup Google Drive and Environment\n",
    "\n",
    "First, we'll mount Google Drive to access and save our data. We'll also install PyHealth from the forked repository and its dependencies. The notebook uses the latest version of PyHealth from https://github.com/naveenkcb/PyHealth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f130a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install PyHealth from your forked repository\n",
    "!pip install git+https://github.com/naveenkcb/PyHealth.git\n",
    "# Install other required packages\n",
    "!pip install torch scikit-learn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c5176",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries and Setup Configuration\n",
    "\n",
    "Now we'll import the necessary libraries and set up our configuration for the MICRON model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "from pyhealth.models import MICRON\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics import multilabel_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configuration\n",
    "MIMIC3_PATH = \"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III\\\"  # Update this path to your MIMIC-III data location\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577fe5a",
   "metadata": {},
   "source": [
    "## 3. Load and Process MIMIC-III Dataset\n",
    "\n",
    "We'll load the MIMIC-III dataset using PyHealth's built-in dataset loader and prepare it for training. The dataset will be processed to include patient diagnoses, procedures, and medications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIMIC-III dataset\n",
    "dataset = MIMIC3Dataset(\n",
    "    root=MIMIC3_PATH,\n",
    "    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],\n",
    "    code_mapping={\"ICD9CM\": \"CCSCM\", \"ATC\": \"ATC\"},\n",
    "    refresh_cache=False,\n",
    ")\n",
    "\n",
    "# Define the dataset schema\n",
    "input_schema = {\n",
    "    \"conditions\": \"sequence\",\n",
    "    \"procedures\": \"sequence\",\n",
    "}\n",
    "output_schema = {\n",
    "    \"drugs\": \"multilabel\"\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.split([\"train\", \"val\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863949d",
   "metadata": {},
   "source": [
    "## 4. Initialize and Configure MICRON Model\n",
    "\n",
    "Now we'll set up the MICRON model with appropriate hyperparameters for drug recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_params = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"lam\": 0.1  # Regularization parameter for reconstruction loss\n",
    "}\n",
    "\n",
    "# Initialize MICRON model\n",
    "model = MICRON(\n",
    "    dataset=train_dataset,\n",
    "    **model_params\n",
    ").to(DEVICE)\n",
    "\n",
    "# Configure trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    metrics=[multilabel_metrics],\n",
    "    train_loader_params={\"batch_size\": 32, \"shuffle\": True},\n",
    "    val_loader_params={\"batch_size\": 32, \"shuffle\": False},\n",
    "    test_loader_params={\"batch_size\": 32, \"shuffle\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209841fe",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Let's train the MICRON model on our processed MIMIC-III dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=10,\n",
    "    monitor=\"val_jaccard_macro\"\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/micron_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e52d9",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Finally, let's evaluate our trained model on the test set and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and actual values\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Forward pass\n",
    "        output = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = output.logits.argmax(dim=-1)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (all_preds == all_labels).mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results using a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47925060",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully implemented and trained a MICRON model for drug recommendation using the MIMIC-III dataset. The model's performance can be evaluated using the metrics above:\n",
    "\n",
    "1. Accuracy: Shows the overall correct prediction rate\n",
    "2. Precision: Indicates how many of the predicted drugs were actually correct\n",
    "3. Recall: Shows how many of the actual drugs were correctly predicted\n",
    "4. F1 Score: The harmonic mean of precision and recall\n",
    "\n",
    "The confusion matrix visualization helps us understand where the model performs well and where it might need improvement. The training loss plot shows how the model learned over time.\n",
    "\n",
    "Next steps could include:\n",
    "- Hyperparameter tuning to improve performance\n",
    "- Testing with different model architectures\n",
    "- Analyzing specific cases where the model performs well or poorly\n",
    "- Incorporating additional patient features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
